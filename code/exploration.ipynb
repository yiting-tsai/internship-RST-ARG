{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import graphviz\n",
    "\n",
    "f_Nodes= open('./Nodes.dbm','rb')\n",
    "f_Concepts= open('./ConceptsStr.dbm','rb')\n",
    "f_perepick= open('./PeresDeConcepts.dbm','rb')\n",
    "f_filspick= open('./FilsDeConcepts.dbm','rb')\n",
    "f_atteqpick= open('./AttEquivalents.dbm','rb')\n",
    "f_objeqpick= open('./ObjEquivalents.dbm','rb')\n",
    "f_AttPereInit= open('./AttPeresImmediats.dbm','rb')\n",
    "f_AttPereTransitive= open('./AttPereFermTransitive.dbm','rb')\n",
    "f_ListePaires= open('./ListeDesPaires.dbm','rb')\n",
    "f_AttOcc= open('./OccurenceDesAttributs.dbm','rb')\n",
    "f_dotDict= open('./dotDict.dbm','rb')\n",
    "\n",
    "Nodes=pickle.load(f_Nodes)\n",
    "Concepts=pickle.load(f_Concepts)\n",
    "pere1=pickle.load(f_perepick)\n",
    "fils= pickle.load(f_filspick)\n",
    "#EquiAtt=pickle.load(f_atteqpick)\n",
    "#EquiObj= pickle.load(f_objeqpick)\n",
    "pereimmediat= pickle.load(f_AttPereInit)\n",
    "pere= pickle.load(f_AttPereTransitive)\n",
    "PairFullList= pickle.load(f_ListePaires)\n",
    "Att_cpt=pickle.load(f_AttOcc)\n",
    "dotDict=pickle.load(f_dotDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "appeared_att_set=set()\n",
    "for k,v in Nodes.items():\n",
    "    for att in v[6]:\n",
    "        appeared_att_set.add(att)\n",
    "        \n",
    "## subgraphs that do NOT appear in the concepts\n",
    "attribute_ls=list(dotDict.keys())\n",
    "no_appeared_att_set=set()\n",
    "for relation in attribute_ls:\n",
    "    if str(relation) not in appeared_att_set:\n",
    "        no_appeared_att_set.add(relation)\n",
    "#len(no_appeared_att_set) -> 2428\n",
    "\n",
    "## subgraphs that DO appear in the concepts\n",
    "is_appeared_att_set=set()\n",
    "for relation in attribute_ls:\n",
    "    if str(relation) in appeared_att_set:\n",
    "        is_appeared_att_set.add(relation)\n",
    "#len(is_appeared_att_set) -> 1278\n",
    "\n",
    "## make clean appeared_att_dotDict\n",
    "appeared_att_dotDict = dotDict.copy()\n",
    "for k,v in dotDict.items():\n",
    "    if k not in is_appeared_att_set:\n",
    "        appeared_att_dotDict.pop(k)\n",
    "#len(appeared_att_dotDict) -> 1278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4e32e3f6624a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msingle_attribute_concept_ls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmulti_attribute_concept_ls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msingle_attribute_concept_ls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_files' is not defined"
     ]
    }
   ],
   "source": [
    "### have the functions run first below \"Local attributes with the most complex structural patterns in a concept\"\n",
    "## list of concepts containing single or mutliple local attribute\n",
    "path_name='HierarchiesBtwAttributesPerConcepts'\n",
    "single_attribute_concept_ls=[]\n",
    "multi_attribute_concept_ls=[]\n",
    "for file in get_files(path_name):\n",
    "    if len(Nodes[file[3:-7]][2]) == 1:\n",
    "        single_attribute_concept_ls.append(file[3:-7])\n",
    "    else:\n",
    "        multi_attribute_concept_ls.append(file[3:-7])\n",
    "# len(single_attribute_concept_ls) -> 190\n",
    "# len(multi_attribute_concept_ls) -> 189\n",
    "\n",
    "## list of concepts containing none of local attributes but inherited ones\n",
    "only_inherited_concepts=[]\n",
    "for c in multi_attribute_concept_ls:\n",
    "    if c not in concept_local_dict:\n",
    "        only_inherited_concepts.append(c)\n",
    "# len(only_inherited_concepts) -> 9 ['89', '92', '14', '73', '17', '199', '148', '93', '121']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make set of attributes containing only single r_multi and a_joint node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_multin_joint_node_ls=set()\n",
    "multi_multin_joint_node_one_rel_ls=set()\n",
    "for k,v in appeared_att_dotDict.items():\n",
    "    r=0\n",
    "    a=0\n",
    "    cache_ls=[]\n",
    "    ## see how many a_joint or r_multi nodes are there in a subgraph\n",
    "    for rel in dotDict[k][0]:\n",
    "        if re.search(r'a\\_joint\\d{1,3}',rel):\n",
    "            cache_ls.append(re.search(r'a\\_joint\\d{1,3}',rel).group(0))\n",
    "        if re.search(r'r_multi\\d{1,3}',rel):\n",
    "            cache_ls.append(re.search(r'r_multi\\d{1,3}',rel).group(0))\n",
    "            \n",
    "    ## see how many [label=\"joint-a\"] or RST multinuclear relations are pointing toward the a_joint or r_multi nodes\n",
    "    # if single node only\n",
    "    if len(cache_ls) == 1:\n",
    "        # ARG\n",
    "        if re.search(r'a',cache_ls[0]):\n",
    "            for rel in dotDict[k][1]:\n",
    "                if re.search(r'\\[label=\"joint-a\"\\]',rel[2]):\n",
    "                    a+=1\n",
    "        # RST\n",
    "        elif re.search(r'r',cache_ls[0]):\n",
    "            for rel in dotDict[k][1]:\n",
    "                if re.search(r'r_multi[\\d{1,4}]?',rel[1]):\n",
    "                    r+=1\n",
    "        # add it to the list of single node \n",
    "        if r == 1 or a == 1:\n",
    "            single_multin_joint_node_ls.add(k)\n",
    "            \n",
    "    # if multi nodes, inspect relations pointing toward them                \n",
    "    elif len(cache_ls) > 1:\n",
    "        # ARG\n",
    "        if re.search(r'a',cache_ls[0]):\n",
    "            cache_dict=dict()\n",
    "            for joint_node in cache_ls:\n",
    "                joint_node_digit = str(joint_node[-1])\n",
    "                joint_node_search_str = 'a\\_joint' + joint_node_digit\n",
    "                joint_search_compile = re.compile(joint_node_search_str)\n",
    "                count=0\n",
    "                for link in dotDict[k][1]:\n",
    "                    if re.search(joint_search_compile,link[1]) and re.search(r'\\[label=\"joint-a\"\\]',link[2]):\n",
    "                        count+=1\n",
    "                cache_dict[joint_node]=count\n",
    "                for jo,co in cache_dict.items():\n",
    "                    if co == 1: \n",
    "                        multi_multin_joint_node_one_rel_ls.add(k)\n",
    "        # RST\n",
    "        elif re.search(r'r',cache_ls[0]):\n",
    "            cache_dict=dict()\n",
    "            for multi_node in cache_ls:\n",
    "                multi_node_digit = str(multi_node[-1])\n",
    "                multi_node_search_str = 'r\\_multi' + multi_node_digit\n",
    "                multi_search_compile = re.compile(multi_node_search_str)\n",
    "                count=0\n",
    "                cache_dict=dict()\n",
    "                for link in dotDict[k][1]:\n",
    "                    if re.search(multi_search_compile,link[1]):\n",
    "                        count+=1\n",
    "                cache_dict[multi_node]=count\n",
    "                for jo,co in cache_dict.items():\n",
    "                    if co == 1:\n",
    "                        multi_multin_joint_node_one_rel_ls.add(k)\n",
    "\n",
    "# len(single_multin_joint_node_ls) -> 197\n",
    "# len(multi_multin_joint_node_one_rel_ls) -> 216\n",
    "\n",
    "## see how many are there of ARG and RST in the set of relations conctaing single a_joint or r_multi node\n",
    "s_a_ls=[]\n",
    "s_r_ls=[]\n",
    "for att in single_multin_joint_node_ls:\n",
    "    if re.match(r'a\\d{1,4}',att):s_a_ls.append(att)\n",
    "    if re.match(r'r\\d{1,4}',att):s_r_ls.append(att)\n",
    "# len(s_a_ls)-> 53\n",
    "# len(s_r_ls) -> 144\n",
    "## see how many are there of ARG and RST in the set of relations conctaing multiple a_joint or r_multi nodes but one relation pointing toward them\n",
    "m_a_ls=[]\n",
    "m_r_ls=[]\n",
    "for att in multi_multin_joint_node_one_rel_ls:\n",
    "    if re.match(r'a\\d{1,4}',att):m_a_ls.append(att)\n",
    "    if re.match(r'r\\d{1,4}',att):m_r_ls.append(att)\n",
    "# len(m_a_ls)-> 114\n",
    "# len(m_r_ls) -> 102"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make set of attributes containing only single a_rel node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arel_node_ls=set()\n",
    "for k,v in appeared_att_dotDict.items():\n",
    "    if re.search('a',k):\n",
    "    ## see how many a_joint or r_multi nodes are there in a subgraph\n",
    "    for rel in dotDict[k][1]:\n",
    "        if re.search(r'a\\_rel\\d{1,3}',rel[0]):\n",
    "            rel_str=re.search(r'a\\_rel\\d{1,3}',rel).group(0)\n",
    "            \n",
    "        if re.search(r'r_multi\\d{1,3}',rel):\n",
    "            cache_ls.append(re.search(r'r_multi\\d{1,3}',rel).group(0))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('r_0', 'r_multi0', '[label=conjunction]')\n"
     ]
    }
   ],
   "source": [
    "for k,v in appeared_att_dotDict.items():\n",
    "    print(appeared_att_dotDict[k][1][0])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make set of argumentation attribute containing only a_joint node and join relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_only_ls=set()\n",
    "joint_not_only_ls=set()\n",
    "for k,v in appeared_att_dotDict.items():\n",
    "    intermediate_ls = list()\n",
    "    for rel in dotDict[k][1]:\n",
    "        intermediate_ls.append(str(rel[2]))\n",
    "    if all(label == '[label=\"joint-a\"]' for label in intermediate_ls):joint_only_ls.add(k)\n",
    "    elif any(label == '[label=\"joint-a\"]' for label in intermediate_ls):joint_not_only_ls.add(k)\n",
    "#len(joint_only_ls) -> 3 (a17,a19,a311)\n",
    "#len(joint_not_only_ls) -> 320\n",
    "\n",
    "## remove attribute only with one single joint node\n",
    "to_discard_ls=[]\n",
    "for rel in joint_not_only_ls:\n",
    "    if rel in single_multin_joint_node_ls:\n",
    "        to_discard_ls.append(rel)\n",
    "for to_discard in to_discard_ls:\n",
    "    joint_not_only_ls.discard(to_discard)\n",
    "#len(to_discard_ls) -> 53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local attributes with the most complex structural patterns in a concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(path_name='HierarchiesBtwAttributesPerConcepts'):\n",
    "    '''\n",
    "    INPUT \n",
    "    path_name: str\n",
    "    ---\n",
    "    OUTPUT\n",
    "    list_of_files: list of filenames of matched string\n",
    "    '''\n",
    "    directory = os.fsencode(path_name)\n",
    "    list_of_files = [f.decode(\"utf-8\") for f in os.listdir(directory) if re.match(r'Cpt\\d{1,4}P-H\\.dot', f.decode(\"utf-8\"))]\n",
    "    #len(list_of_files) -> 379\n",
    "    return list_of_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(path_name, filename):\n",
    "    '''\n",
    "    INPUT \n",
    "    path_name: str\n",
    "    filemame: str\n",
    "    ---\n",
    "    OUTPUT\n",
    "    digraph_tree_list: list of elements of digraph G \n",
    "    '''\n",
    "    file=graphviz.Source.from_file(path_name+'/'+filename)\n",
    "    f_source= file.source\n",
    "    digraph_tree_list =[]\n",
    "    for line in f_source.splitlines():\n",
    "        digraph_tree_list.append(line)\n",
    "    return digraph_tree_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_attributes(digraph_tree_list):\n",
    "    '''\n",
    "    INPUT \n",
    "    digraph_tree_list: list of elements of digraph G \n",
    "    ---\n",
    "    OUTPUT\n",
    "    list_of_local_attributes: list of local attributes in strings\n",
    "    '''\n",
    "    list_of_local_attributes=[]\n",
    "    for i in range(len(digraph_tree_list)):\n",
    "        if re.search(r'fillcolor=khaki1',digraph_tree_list[i]):\n",
    "            katt =re.search(r'\\(?(a|r)+\\d{1,4}\\,?((a|r)+\\d{1,4})?\\)?',digraph_tree_list[i]).group(0)\n",
    "            list_of_local_attributes.append(katt)\n",
    "    return list_of_local_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_of_linkage(digraph_tree_list):\n",
    "    '''\n",
    "    INPUT \n",
    "    digraph_tree_list: list of elements of digraph G \n",
    "    ---\n",
    "    OUTPUT\n",
    "    sliced digraph_tree_list: list of elements of digraph G which contain a '->' arrow as edges\n",
    "    '''\n",
    "    ind=[]\n",
    "    a=1\n",
    "    for i in range(len(digraph_tree_list)):\n",
    "        if re.search(r'->',digraph_tree_list[i]):\n",
    "            ind.append(a)\n",
    "            a+=1\n",
    "            # 1 digraph_tree_list[ind.index(1)]\n",
    "            # a digraph_tree_list[ind.index(a-1)]\n",
    "        else:\n",
    "            ind.append(0)\n",
    "    sliced_digraph_tree_list= digraph_tree_list[ind.index(1):ind.index(a-1)+1]\n",
    "    return sliced_digraph_tree_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_attribute_and_linkage(list_of_local_attributes, sliced_digraph_tree_list):\n",
    "    '''\n",
    "    INPUT \n",
    "    sliced_digraph_tree_list: list of elements of digraph G which contain a '->' arrow as edges \n",
    "    ---\n",
    "    OUTPUT\n",
    "    match_dict: local attributes as keys and their matched linkages as values\n",
    "    '''\n",
    "    match_dict=dict()\n",
    "    \n",
    "    ## concstruct re,compile string for search\n",
    "    for local in list_of_local_attributes:\n",
    "        if re.search(r'\\(',local):\n",
    "            local_linkage = ' -> ' + '\"\\\\('+str(local[1:-1])+'\\\\)\"'\n",
    "        else:\n",
    "            local_linkage = ' -> ' + '\"'+str(local)+'\"'\n",
    "        # compile    \n",
    "        local_linkage_compile= re.compile(local_linkage)\n",
    "        ## start matching\n",
    "        cache_ls=[]\n",
    "        for linkage in sliced_digraph_tree_list:\n",
    "            if re.search(local_linkage_compile,linkage):\n",
    "                cache_ls.append(linkage)\n",
    "        # build output dict {local attribute : list of matched linkage}\n",
    "        match_dict[local]=cache_ls\n",
    "    \n",
    "    return match_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_name='HierarchiesBtwAttributesPerConcepts'\n",
    "concept_local_dict=dict()\n",
    "\n",
    "for file in get_files(path_name):\n",
    "    if len(Nodes[file[3:-7]][2]) == 1:\n",
    "        continue\n",
    "    digraph_tree_list= read_files(path_name,file)\n",
    "    list_of_local_attributes=get_local_attributes(digraph_tree_list)\n",
    "    sliced_digraph_tree_list=get_index_of_linkage(digraph_tree_list)\n",
    "    match_dict=get_local_attribute_and_linkage(list_of_local_attributes, sliced_digraph_tree_list)\n",
    "    for k,v in match_dict.items():\n",
    "        k_ls=[]\n",
    "        if len(v)==0:\n",
    "            k_ls.append(k)\n",
    "            concept_local_dict[file[3:-7]]= k_ls\n",
    "# len(concept_local_dict) -> 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_most_complex_local_attribute(match_dict):\n",
    "    '''\n",
    "    The most complex local attribute is not directed/pointed by any other nodes at all, thus is the\n",
    "    ---\n",
    "    INPUT \n",
    "    match_dict: local attributes as keys and their matched linkages as values \n",
    "    ---\n",
    "    OUTPUT\n",
    "    most_complex_local_attribute: string of the most complex local attribute\n",
    "    '''\n",
    "#    for k, v in match_dict.items():\n",
    "#        if len(v)==0:\n",
    "#            most_complex_local_attribute = k\n",
    "#    return most_complex_local_attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching specific relation configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all relation labels\n",
    "a_label_name=set()\n",
    "r_label_name=set()\n",
    "for k,v in appeared_att_dotDict.items():\n",
    "    # ARG\n",
    "    if re.search(r'a',k):\n",
    "        for link in v[1]:\n",
    "            if link[2] not in a_label_name:\n",
    "                a_label_name.add(link[2])\n",
    "    # RST\n",
    "    if re.search(r'r',k):\n",
    "        for link in v[1]:\n",
    "            if link[2] not in r_label_name:\n",
    "                r_label_name.add(link[2])   \n",
    "                \n",
    "# len(a_label_name) -> 6\n",
    "# len(r_label_name) -> 26\n",
    "a_label_name_unique_ls=list(a_label_name)\n",
    "r_label_name_unique_ls=list(r_label_name)\n",
    "\n",
    "a_name_unique_set=set()\n",
    "r_name_unique_set=set()\n",
    "for lab in a_label_name_unique_ls:\n",
    "    a_name_unique_set.add(lab[7:-1])\n",
    "for lab in r_label_name_unique_ls:\n",
    "    r_name_unique_set.add(lab[7:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rebuttal-Undercutter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting storing set\n",
    "rebuttal_and_undercutter_set=set() \n",
    "for k,v in appeared_att_dotDict.items(): # pre-filtered dot dictionary\n",
    "    # ARG\n",
    "    if re.search(r'a',k):\n",
    "        # for loop on every edge description in dot source file\n",
    "        for rel in v[1]: \n",
    "            # if the reception node is named 'a_rel' \n",
    "            # and incoming relation is Undercutter\n",
    "            if re.search(r'a_rel\\d{1,3}',rel[1]) and re.search(r'und',rel[2]):\n",
    "            \n",
    "                # another for loop on every edge description in dot source file\n",
    "                for rel in v[1]:\n",
    "                    # if the projection node is also a 'a_rel'\n",
    "                    # and also the projecting relation is Rebuttal\n",
    "                    if re.search(r'a_rel\\d{1,3}',rel[0]) and re.search(r'reb',rel[2]):\n",
    "                        # when two requirements meet, add the storing set\n",
    "                        rebuttal_and_undercutter_set.add(k)\n",
    "\n",
    "# searching each RST correspondences of matched-pattern argumentation attributes\n",
    "matched_pair_dict=dict()\n",
    "for rel in rebuttal_and_undercutter_set:\n",
    "    matched_pair_list=[]\n",
    "    re_compile = re.compile(rel)\n",
    "    \n",
    "    # search in the list of alignment pair\n",
    "    for el in PairFullList:\n",
    "        if re.search(re_compile,el):\n",
    "            matched_pair_list.append(el)\n",
    "    matched_pair_dict[rel]=matched_pair_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### more specific complex configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuttal-undercutter-rebuttal\n",
    "rebuttal_and_undercutter_rebuttal_set=set()\n",
    "for att in rebuttal_and_undercutter_set:\n",
    "    for edge in appeared_att_dotDict[att][1]:\n",
    "        if re.search(r'a_rel\\d{1,3}',edge[0]) and re.search(r'reb',edge[2]):\n",
    "            recep_node = re.compile(str(edge[0]))\n",
    "            for ed in appeared_att_dotDict[att][1]:\n",
    "                if re.search(recep_node,ed[1]) and re.search(r'reb',ed[2]):\n",
    "                    rebuttal_and_undercutter_rebuttal_set.add(att)\n",
    "# len(rebuttal_and_undercutter_rebuttal_set) -> 59\n",
    "\n",
    "# rebuttal-undercutter-undercutter\n",
    "rebuttal_and_undercutter_undercutter_set=set()\n",
    "for att in rebuttal_and_undercutter_set:\n",
    "    for edge in appeared_att_dotDict[att][1]:\n",
    "        if re.search(r'a_rel\\d{1,3}',edge[0]) and re.search(r'reb',edge[2]):\n",
    "            recep_node = re.compile(str(edge[0]))\n",
    "            for ed in appeared_att_dotDict[att][1]:\n",
    "                if re.search(recep_node,ed[1]) and re.search(r'und',ed[2]):\n",
    "                    rebuttal_and_undercutter_undercutter_set.add(att)\n",
    "# len(rebuttal_and_undercutter_undercutter_set) -> 2\n",
    "\n",
    "# rebuttal-undercutter-support\n",
    "rebuttal_and_undercutter_support_set=set()\n",
    "for att in rebuttal_and_undercutter_set:\n",
    "    for edge in appeared_att_dotDict[att][1]:\n",
    "        if re.search(r'a_rel\\d{1,3}',edge[0]) and re.search(r'reb',edge[2]):\n",
    "            recep_node = re.compile(str(edge[0]))\n",
    "            for ed in appeared_att_dotDict[att][1]:\n",
    "                if re.search(recep_node,ed[1]) and re.search(r'sup',ed[2]):\n",
    "                    rebuttal_and_undercutter_support_set.add(att)\n",
    "# len(rebuttal_and_undercutter_support_set) -> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuttal-undercutter--support\n",
    "rebuttal_and_undercutter_fsupport_set=set()\n",
    "for att in rebuttal_and_undercutter_set:\n",
    "    for edge in appeared_att_dotDict[att][1]:\n",
    "        if re.search(r'a_rel\\d{1,3}',edge[1]) and re.search(r'und',edge[2]):\n",
    "            recep_node = re.compile(str(edge[0]))\n",
    "            for ed in appeared_att_dotDict[att][1]:\n",
    "                if re.search(recep_node,ed[1]) and re.search(r'sup',ed[2]):\n",
    "                    rebuttal_and_undercutter_fsupport_set.add(att)\n",
    "# len(rebuttal_and_undercutter_fsupport_set) -> 8\n",
    "\n",
    "# find this configuration pair in RST\n",
    "smatched_pair_dict=dict()\n",
    "for rel in rebuttal_and_undercutter_fsupport_set:\n",
    "    matched_pair_list=[]\n",
    "    re_compile = re.compile(rel)\n",
    "    \n",
    "    # search in the list of alignment pair\n",
    "    for el in PairFullList:\n",
    "        if re.search(re_compile,el):\n",
    "            matched_pair_list.append(el)\n",
    "    smatched_pair_dict[rel]=matched_pair_list\n",
    "\n",
    "#---------------------------------------------------#\n",
    "# rebuttal-undercutter--rebuttal\n",
    "rebuttal_and_undercutter_frebuttal_set=set()\n",
    "for att in rebuttal_and_undercutter_set:\n",
    "    for edge in appeared_att_dotDict[att][1]:\n",
    "        if re.search(r'a_rel\\d{1,3}',edge[1]) and re.search(r'und',edge[2]):\n",
    "            recep_node = re.compile(str(edge[0]))\n",
    "            for ed in appeared_att_dotDict[att][1]:\n",
    "                if re.search(recep_node,ed[1]) and re.search(r'reb',ed[2]):\n",
    "                    rebuttal_and_undercutter_frebuttal_set.add(att)\n",
    "# len(rebuttal_and_undercutter_frebuttal_set) -> 0\n",
    "\n",
    "#---------------------------------------------------#\n",
    "# rebuttal-undercutter--undcutter\n",
    "rebuttal_and_undercutter_fundcutter_set=set()\n",
    "for att in rebuttal_and_undercutter_set:\n",
    "    for edge in appeared_att_dotDict[att][1]:\n",
    "        if re.search(r'a_rel\\d{1,3}',edge[1]) and re.search(r'und',edge[2]):\n",
    "            recep_node = re.compile(str(edge[0]))\n",
    "            for ed in appeared_att_dotDict[att][1]:\n",
    "                if re.search(recep_node,ed[1]) and re.search(r'und',ed[2]):\n",
    "                    rebuttal_and_undercutter_fundcutter_set.add(att)\n",
    "# len(rebuttal_and_undercutter_fundcutter_set) -> 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support-Undercutter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## support and undercutter\n",
    "support_and_undercutter_set=set()\n",
    "for k,v in appeared_att_dotDict.items():\n",
    "    if re.search(r'a',k):\n",
    "        for rel in v[1]:\n",
    "            if re.search(r'a_rel\\d{1,3}',rel[1]) and re.search(r'und',rel[2]):\n",
    "                for rel in v[1]:\n",
    "                    if re.search(r'a_rel\\d{1,3}',rel[0]) and re.search(r'sup',rel[2]):\n",
    "                        support_and_undercutter_set.add(k)\n",
    "# len(support_and_undercutter_set) -> 30\n",
    "\n",
    "## remove single a_joint or JOIN relation only subgraphs\n",
    "overlap1=support_and_undercutter_set.intersection(single_multin_joint_node_ls) # len(overlap1) -> 1\n",
    "overlap2=support_and_undercutter_set.intersection(multi_multin_joint_node_one_rel_ls) # len(overlap2) -> 0\n",
    "overlap=overlap1.union(overlap2) # len(overlap) -> \n",
    "for lap in overlap:\n",
    "    if lap in support_and_undercutter_set:\n",
    "        support_and_undercutter_set.remove(lap) # len(rebuttal_and_undercutter_C_set) -> 29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count mapping relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Firstly search for its alignment in matched_pair_dict which defined previously while matching specific configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Counting RST's relaiton of an ARG alignment \n",
    "rname_unique_dict=dict.fromkeys(r_name_unique_set, 0) \n",
    "\n",
    "# specify the ARG attribute in between 'a62'\n",
    "for pair in matched_pair_dict['a62']:\n",
    "    if re.search(r'r\\d{1,4}',pair):\n",
    "        r_att=re.search(r'r\\d{1,4}',pair).group(0)\n",
    "        for edge in appeared_att_dotDict[r_att][1]:\n",
    "            rname_unique_dict[edge[2][7:-1]] += 1\n",
    "            \n",
    "# descending order to see which relation get mapped the most\n",
    "# sorted(rname_unique_dict.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Counting ARG's relaiton of a RST alignment \n",
    "aname_unique_dict=dict.fromkeys(a_name_unique_set, 0) \n",
    "\n",
    "# specify the RST attribute in between 'r1'\n",
    "for pair in matched_pair_dict['r1']:\n",
    "    if re.search(r'a\\d{1,4}',pair):\n",
    "        r_att=re.search(r'a\\d{1,4}',pair).group(0)\n",
    "        for edge in appeared_att_dotDict[r_att][1]:\n",
    "            aname_unique_dict[edge[2][7:-1]] += 1\n",
    "            \n",
    "# descending order to see which relation get mapped the most\n",
    "# sorted(aname_unique_dict.items(), key=lambda x: x[1], reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
